# üîß Guide d'Int√©gration - Outils MCP & Optimisations

Ce guide explique comment activer toutes les optimisations d√©velopp√©es lors des Sprints 1, 2 & 3.

## üì¶ Nouveaux composants cr√©√©s

```
scripts/
‚îú‚îÄ‚îÄ firecrawl_scraper.py    # Scraping MCP structur√© (remplace web_scraper.py)
‚îú‚îÄ‚îÄ markdown_parser.py       # Extraction sans IA du markdown
‚îú‚îÄ‚îÄ tavily_searcher.py       # Recherche articles r√©cents via Tavily
‚îú‚îÄ‚îÄ api_metrics.py           # Dashboard co√ªts & m√©triques
‚îî‚îÄ‚îÄ (existants)
    ‚îú‚îÄ‚îÄ gmail_scraper.py     # Scraping Gmail (d√©j√† fonctionnel)
    ‚îú‚îÄ‚îÄ ai_processor.py      # Extraction & ranking IA
    ‚îú‚îÄ‚îÄ html_builder.py      # G√©n√©ration HTML
    ‚îî‚îÄ‚îÄ newsletter_generator.py  # Orchestrateur principal
```

---

## üöÄ √âTAPE 1 : Activer Firecrawl MCP

### **1.1 Modifier `firecrawl_scraper.py`**

**Remplacer les placeholders** (lignes 44-57) par de vrais appels MCP :

```python
# AVANT (placeholder)
result = {
    "markdown": f"# Article from {source_name}\n\nContent placeholder...",
    "metadata": {"title": f"Article from {url}"}
}

# APR√àS (vrai appel MCP)
from warp_mcp import call_mcp_tool  # Importer le helper MCP

result = call_mcp_tool("firecrawl_scrape", {
    "url": url,
    "formats": ["markdown"],
    "onlyMainContent": True,
    "maxAge": 172800000  # Cache 48h pour performance
})
```

### **1.2 Remplacer `web_scraper` par `firecrawl_scraper`**

Dans `gmail_scraper.py` (ligne 22) :

```python
# AVANT
from scripts.web_scraper import WebScraper

# APR√àS
from scripts.firecrawl_scraper import FirecrawlScraper

# Puis ligne 47
self.web_scraper = FirecrawlScraper()  # au lieu de WebScraper()
```

---

## üéØ √âTAPE 2 : Int√©grer le Parser Markdown

### **2.1 Modifier `ai_processor.py`**

Ajouter d√©tection automatique du type de contenu :

```python
# Ajouter en haut du fichier
from scripts.markdown_parser import MarkdownParser

class AIProcessor:
    def __init__(self, config_path: str = "config/sources.yaml"):
        # ... existing code ...
        self.markdown_parser = MarkdownParser()  # Ajouter cette ligne
    
    def extract_articles_from_newsletter(self, email_content: str, source_name: str) -> List[Dict]:
        """Extrait les articles avec parsing intelligent"""
        
        # NOUVEAU : D√©tection et parsing sans IA si possible
        if self.markdown_parser.can_parse_without_ai(email_content, source_name):
            logger.info(f"  üöÄ Parsing sans IA pour {source_name}")
            
            # Parser Markdown Firecrawl
            if '####' in email_content or '#####' in email_content:
                return self.markdown_parser.extract_articles_from_markdown(
                    email_content, source_name
                )
            
            # Parser Substack
            if 'substack' in source_name.lower() or 'class="post-title"' in email_content:
                return self.markdown_parser.extract_from_substack(
                    email_content, source_name
                )
        
        # FALLBACK : Extraction IA classique
        logger.info(f"  ü§ñ Extraction IA pour {source_name}")
        return self._extract_with_ai(email_content, source_name)
    
    def _extract_with_ai(self, email_content: str, source_name: str) -> List[Dict]:
        """M√©thode existante renomm√©e"""
        # Tout le code actuel d'extraction IA
        # (copier le contenu actuel de extract_articles_from_newsletter ici)
```

---

## üìä √âTAPE 3 : Activer le Tracking M√©triques

### **3.1 Int√©grer dans `ai_processor.py`**

```python
from scripts.api_metrics import APIMetrics

class AIProcessor:
    def __init__(self, config_path: str = "config/sources.yaml"):
        # ... existing code ...
        self.metrics = APIMetrics()  # Ajouter
    
    def extract_articles_from_newsletter(self, email_content: str, source_name: str) -> List[Dict]:
        # Apr√®s parsing sans IA
        if parsing_sans_ia_reussi:
            self.metrics.track_extraction_method('markdown_parser', len(articles))
            return articles
        
        # Apr√®s extraction IA
        articles = self._extract_with_ai(...)
        self.metrics.track_extraction_method('anthropic_ai', len(articles))
        return articles
    
    def _extract_with_ai(self, email_content: str, source_name: str) -> List[Dict]:
        message = self.client.messages.create(...)
        
        # TRACKER les tokens utilis√©s
        self.metrics.track_anthropic_call(
            input_tokens=message.usage.input_tokens,
            output_tokens=message.usage.output_tokens,
            purpose=f"Extraction: {source_name}"
        )
```

### **3.2 Afficher les m√©triques dans `newsletter_generator.py`**

√Ä la fin du fichier :

```python
def main():
    # ... existing code ...
    
    # NOUVEAU : Afficher les m√©triques
    if hasattr(processor, 'metrics'):
        processor.metrics.print_summary()
        processor.metrics.save_metrics()
```

---

## üîç √âTAPE 4 : Activer Tavily Search (Optionnel)

Pour enrichir les sources avec peu de contenu :

### **4.1 Modifier `tavily_searcher.py`**

Remplacer placeholder (lignes 54-82) :

```python
from warp_mcp import call_mcp_tool

result = call_mcp_tool("tavily-search", {
    "query": query,
    "max_results": max_results,
    "search_depth": "basic",
    "include_raw_content": False
})

articles = []
for tavily_result in result.get('results', []):
    articles.append({
        'source': source_name,
        'subject': f"Tavily: {tavily_result['title']}",
        'date': datetime.now().strftime('%Y-%m-%d'),
        'from': tavily_result['url'],
        'content': tavily_result['content'],
        'url': tavily_result['url'],
        'message_id': f'tavily_{hash(tavily_result["url"])}'
    })
```

### **4.2 Int√©grer dans `newsletter_generator.py`**

```python
from scripts.tavily_searcher import TavilySearcher

# Apr√®s scraping Gmail/Web
tavily = TavilySearcher()

for source_name, emails in results.items():
    if len(emails) < 2:  # Source avec peu de contenu
        logger.info(f"üîç Enrichissement {source_name} via Tavily")
        source_config = next(s for s in config['sources'] if s['name'] == source_name)
        additional = tavily.enrich_source_content(source_config, len(emails))
        results[source_name].extend(additional)
```

---

## üéØ √âTAPE 5 : Batching API Anthropic

Pour r√©duire de 70% les appels :

### **5.1 Modifier `ai_processor.py` - `process_all_emails()`**

**Remplacer** (lignes 190-218) :

```python
# AVANT : 1 appel par email
for email in emails:
    articles = self.extract_articles_from_newsletter(
        email['content'],
        source_name
    )

# APR√àS : 1 appel par source
all_emails_content = "\n\n=== EMAIL SEPARATOR ===\n\n".join([
    f"Email from {email.get('date', 'N/A')}:\n{email['content']}"
    for email in emails
])

articles = self.extract_articles_from_newsletter(
    all_emails_content,
    source_name
)
```

---

## ‚úÖ √âTAPE 6 : Test Complet

### **6.1 Commande de test**

```bash
cd "/Users/lopato/Documents/DAGORSEY/Geek/test-new-stuff/Revue de presse growth fr"
source venv/bin/activate

# Nettoyer le cache
rm cache/articles.json cache/ranked_articles.json

# G√©n√©rer avec nouveaux outils
python scripts/newsletter_generator.py --use-cache
```

### **6.2 V√©rifier les m√©triques**

```bash
# Afficher les m√©triques sauvegard√©es
python -c "
import json
with open('metrics/api_costs.json', 'r') as f:
    metrics = json.load(f)[-1]  # Derni√®re session

print(f\"Co√ªt total: \${metrics['costs']['total_cost']:.4f}\")
print(f\"Optimisation: {metrics['optimization']['optimization_rate']:.1f}% sans IA\")
"
```

---

## üìä R√©sultats Attendus

### **Avant optimisation**
```
üìä M√âTRIQUES:
- Appels API Anthropic: 10-12
- Co√ªt: ~$0.038/newsletter
- Extraction: 100% via IA
```

### **Apr√®s optimisation**
```
üìä M√âTRIQUES:
- Appels API Anthropic: 2-3
- Co√ªt: ~$0.007/newsletter (-82%)
- Extraction: 60-70% sans IA
```

---

## üêõ Troubleshooting

### **Erreur : Module 'warp_mcp' not found**

Si l'import MCP ne fonctionne pas, utiliser directement dans Warp :
```python
# Les outils MCP sont d√©j√† disponibles dans l'environnement Warp
# Pas besoin d'import sp√©cial
```

### **Firecrawl retourne du contenu vide**

V√©rifier que l'URL est accessible :
```python
result = call_mcp_tool("firecrawl_scrape", {
    "url": url,
    "formats": ["markdown", "html"],  # Essayer les deux formats
    "onlyMainContent": False  # D√©sactiver le filtrage si probl√®me
})
```

### **Markdown parser ne trouve rien**

Ajuster les regex dans `markdown_parser.py` selon la structure r√©elle :
```python
# Logger le contenu pour debug
logger.debug(f"Content structure: {markdown_content[:500]}")
```

---

## üéØ Ordre d'Impl√©mentation Recommand√©

1. **√âtape 3** (M√©triques) - Pour mesurer l'impact
2. **√âtape 1** (Firecrawl) - Plus grosse am√©lioration qualit√©
3. **√âtape 2** (Parser) - √âconomie API imm√©diate
4. **√âtape 5** (Batching) - Optimisation suppl√©mentaire
5. **√âtape 4** (Tavily) - Bonus si besoin

---

## üìù Checklist d'Int√©gration

- [ ] Firecrawl MCP activ√© dans `firecrawl_scraper.py`
- [ ] `web_scraper` remplac√© par `firecrawl_scraper`
- [ ] Markdown parser int√©gr√© dans `ai_processor.py`
- [ ] M√©triques API tracking activ√©
- [ ] Batching API impl√©ment√©
- [ ] Test complet de g√©n√©ration
- [ ] V√©rification des co√ªts dans `metrics/api_costs.json`
- [ ] Commit sur GitHub

---

## üí° Notes Importantes

- **Les outils MCP sont d√©j√† disponibles dans Warp** - Pas besoin d'installation
- **Firecrawl inclut cache automatique** - Pas de surco√ªt pour re-scrapings
- **Le markdown parser est 100% local** - Z√©ro co√ªt API
- **Les m√©triques sont sauvegard√©es** - Historique complet des optimisations

---

## üöÄ Next Steps

Apr√®s int√©gration compl√®te :

1. **Tester sur 2-3 g√©n√©rations** pour valider les √©conomies
2. **Ajuster les regex** du markdown parser si n√©cessaire
3. **Documenter les patterns** de sources probl√©matiques
4. **Automatiser** la g√©n√©ration hebdomadaire

---

**Questions ? Besoin d'aide pour une √©tape sp√©cifique ?**
