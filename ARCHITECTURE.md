# Architecture d'Automatisation - Growth Weekly

## ğŸ¯ Vue d'ensemble

Ce document dÃ©crit l'architecture complÃ¨te du systÃ¨me d'automatisation pour gÃ©nÃ©rer la newsletter Growth Weekly hebdomadaire en franÃ§ais.

---

## ğŸ“ Structure du Projet (ProposÃ©e)

```
Growth-newsletter/
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md           # Ce fichier
â”œâ”€â”€ ğŸ“„ WARP.md                   # Documentation pour Warp
â”œâ”€â”€ ğŸ“„ README.md                 # Guide utilisateur
â”œâ”€â”€ ğŸ“„ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ .env.example              # Template variables d'environnement
â”œâ”€â”€ ğŸ“„ .gitignore                # Fichiers Ã  ignorer
â”‚
â”œâ”€â”€ ğŸ“ config/
â”‚   â”œâ”€â”€ sources.json             # Configuration des 13+ sources
â”‚   â””â”€â”€ prompts.json             # Prompts IA pour rÃ©sumÃ©/traduction
â”‚
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ newsletter_generator.py  # Script principal
â”‚   â”œâ”€â”€ scraper.py              # Module de scraping
â”‚   â”œâ”€â”€ ai_processor.py         # Traitement IA (rÃ©sumÃ©/traduction)
â”‚   â””â”€â”€ html_builder.py         # GÃ©nÃ©ration HTML
â”‚
â”œâ”€â”€ ğŸ“ templates/
â”‚   â””â”€â”€ newsletter_template.html # Template HTML
â”‚
â”œâ”€â”€ ğŸ“ output/
â”‚   â””â”€â”€ newsletters/            # Newsletters gÃ©nÃ©rÃ©es (git ignored)
â”‚
â””â”€â”€ ğŸ“ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ weekly_newsletter.yml # GitHub Actions (optionnel)
```

---

## ğŸ”„ Flux de Travail (Workflow)

### Ã‰tape 1: Collecte des Sources
```
[13+ Sources Growth] 
    â†“
[Scraper Module]
    â†“
[DonnÃ©es brutes JSON]
```

**Comment Ã§a marche:**
- Le script lit `config/sources.json`
- Pour chaque source, il utilise les MCP tools disponibles:
  - `firecrawl_scrape` pour les pages web
  - `tavily-search` pour rechercher du contenu rÃ©cent
  - `fetch` comme fallback
- Collecte les articles publiÃ©s dans les 7 derniers jours
- Sauvegarde en JSON: `{url, title, content, source, date}`

### Ã‰tape 2: Traitement IA
```
[DonnÃ©es brutes]
    â†“
[AI Processor]
    â†“
[Articles rÃ©sumÃ©s + traduits + catÃ©gorisÃ©s]
```

**Comment Ã§a marche:**
- Pour chaque article collectÃ©:
  1. **RÃ©sumÃ©**: GÃ©nÃ¨re un rÃ©sumÃ© de 2 lignes en franÃ§ais
  2. **CatÃ©gorisation**: DÃ©termine si c'est Critique/Important/Bon Ã  Savoir
  3. **Scoring**: Attribue un score d'importance (1-10)
- Utilise des prompts optimisÃ©s pour cohÃ©rence
- Retourne un JSON structurÃ©

### Ã‰tape 3: Curation & Ã‰quilibrage
```
[Articles traitÃ©s]
    â†“
[Balance Algorithm]
    â†“
[25+ articles Ã©quilibrÃ©s par source]
```

**Comment Ã§a marche:**
- Trie par score d'importance
- Assure reprÃ©sentation Ã©quilibrÃ©e des sources (algorithme de distribution)
- SÃ©lectionne top 25+ articles
- RÃ©partit: 8 Critique, 8 Important, 9+ Bon Ã  Savoir

### Ã‰tape 4: GÃ©nÃ©ration HTML
```
[Articles finaux]
    â†“
[HTML Builder]
    â†“
[Newsletter complÃ¨te growth-weekly-YYYY-MM-DD.html]
```

**Comment Ã§a marche:**
- Charge le template HTML
- Injecte les donnÃ©es dans la structure JavaScript
- GÃ©nÃ¨re le footer avec les sources utilisÃ©es
- Sauvegarde dans `output/newsletters/`

---

## ğŸ”§ Modules DÃ©taillÃ©s

### 1ï¸âƒ£ `newsletter_generator.py` (Script Principal)

```python
# Pseudo-code de fonctionnement

def main():
    # Configuration
    sources = load_config('config/sources.json')
    end_date = get_last_week_end_date()
    
    # Ã‰tape 1: Scraping
    print("ğŸ“¥ Collecte des sources...")
    raw_articles = scrape_all_sources(sources, days=7)
    save_json(raw_articles, 'cache/raw_articles.json')
    
    # Ã‰tape 2: Traitement IA
    print("ğŸ¤– Traitement IA...")
    processed = ai_process_articles(raw_articles)
    save_json(processed, 'cache/processed_articles.json')
    
    # Ã‰tape 3: Curation
    print("âš–ï¸ Ã‰quilibrage des sources...")
    curated = balance_sources(processed, min_items=25)
    
    # Ã‰tape 4: GÃ©nÃ©ration
    print("ğŸ“° GÃ©nÃ©ration HTML...")
    html = generate_newsletter(curated, end_date)
    save_html(html, f'output/newsletters/growth-weekly-{end_date}.html')
    
    print("âœ… Newsletter gÃ©nÃ©rÃ©e!")
    return html
```

**Arguments de ligne de commande:**
```bash
python scripts/newsletter_generator.py \
    --end-date 2025-10-26 \
    --min-articles 25 \
    --output output/newsletters/ \
    --dry-run  # Mode test sans gÃ©nÃ©ration finale
```

### 2ï¸âƒ£ `scraper.py` (Module de Scraping)

```python
class NewsletterScraper:
    """
    GÃ¨re le scraping de toutes les sources configurÃ©es
    """
    
    def __init__(self, sources_config):
        self.sources = sources_config
        self.mcp_tools = initialize_mcp_tools()
    
    def scrape_source(self, source):
        """
        Scrape une source individuelle
        
        Args:
            source: {
                "name": "Kyle Poyar",
                "type": "substack",
                "url": "https://www.growthunhinged.com",
                "method": "firecrawl_scrape"
            }
        
        Returns:
            List of articles with metadata
        """
        if source['method'] == 'firecrawl_scrape':
            return self._scrape_with_firecrawl(source)
        elif source['method'] == 'tavily-search':
            return self._search_with_tavily(source)
        else:
            return self._scrape_with_fetch(source)
    
    def scrape_all_sources(self, days=7):
        """
        Scrape toutes les sources en parallÃ¨le
        """
        # Utilise ThreadPoolExecutor pour parallÃ©lisation
        # Retourne tous les articles des 7 derniers jours
```

**StratÃ©gies par type de source:**
- **Substack** (Kyle, Elena, etc.): `firecrawl_scrape` sur page d'archive
- **Blogs** (Demand Curve): `tavily-search` avec filtres de date
- **Newsletters email**: Instructions pour intÃ©gration Gmail API (optionnel)

### 3ï¸âƒ£ `ai_processor.py` (Traitement IA)

```python
class AIProcessor:
    """
    Traite les articles avec IA pour rÃ©sumÃ©, traduction et catÃ©gorisation
    """
    
    def process_article(self, article):
        """
        Traite un article individuel
        
        Input:
            {
                "url": "...",
                "title": "Your pricing is broken",
                "content": "In just the past few weeks...",
                "source": "Kyle Poyar"
            }
        
        Output:
            {
                "url": "...",
                "title_fr": "Votre pricing est cassÃ©",
                "summary_fr": "Kyle Poyar dÃ©montre...",
                "category": "critical",
                "score": 9.2,
                "source": "Kyle Poyar / Growth Unhinged"
            }
        """
        # 1. RÃ©sumÃ© en franÃ§ais (2 lignes max)
        summary = self._generate_summary(article)
        
        # 2. Traduction du titre
        title = self._translate_title(article['title'])
        
        # 3. CatÃ©gorisation
        category, score = self._categorize(article, summary)
        
        return {
            'url': article['url'],
            'title_fr': title,
            'summary_fr': summary,
            'category': category,
            'score': score,
            'source': article['source']
        }
```

**Prompts utilisÃ©s:**

```json
{
  "summarize": {
    "system": "Tu es un expert en growth marketing. RÃ©sume cet article en franÃ§ais en 2 lignes maximum. Capture l'insight principal et son impact pratique.",
    "template": "Article: {title}\n\nContenu: {content}\n\nRÃ©sumÃ© en franÃ§ais (2 lignes):"
  },
  
  "categorize": {
    "system": "CatÃ©gorise cet article de growth marketing selon son importance.",
    "template": "Titre: {title}\nRÃ©sumÃ©: {summary}\n\nCatÃ©gorie (critical/important/good_to_know) + Score (1-10):"
  }
}
```

### 4ï¸âƒ£ `html_builder.py` (GÃ©nÃ©ration HTML)

```python
def generate_newsletter(articles, week_end_date):
    """
    GÃ©nÃ¨re le HTML final de la newsletter
    
    Args:
        articles: Liste d'articles catÃ©gorisÃ©s et triÃ©s
        week_end_date: Date de fin de semaine (format YYYY-MM-DD)
    
    Returns:
        HTML string complet
    """
    # 1. Charge le template
    template = load_template('templates/newsletter_template.html')
    
    # 2. Organise par catÃ©gorie
    critical = [a for a in articles if a['category'] == 'critical'][:8]
    important = [a for a in articles if a['category'] == 'important'][:8]
    good_to_know = [a for a in articles if a['category'] == 'good_to_know'][:9]
    
    # 3. GÃ©nÃ¨re le JavaScript data
    news_data = {
        'critical': critical,
        'important': important,
        'goodToKnow': good_to_know
    }
    
    # 4. Injecte dans le template
    html = template.replace(
        '/* NEWS_DATA_PLACEHOLDER */',
        f'const newsData = {json.dumps(news_data, ensure_ascii=False)};'
    )
    
    # 5. Met Ã  jour les dates
    week_start = get_week_start_date(week_end_date)
    html = html.replace('{{WEEK_START}}', week_start)
    html = html.replace('{{WEEK_END}}', week_end_date)
    
    # 6. GÃ©nÃ¨re le footer des sources
    sources_used = list(set([a['source'] for a in articles]))
    sources_html = '\n'.join([f'<li>â€¢ {s}</li>' for s in sorted(sources_used)])
    html = html.replace('{{SOURCES_LIST}}', sources_html)
    
    return html
```

---

## âš™ï¸ Configuration: `config/sources.json`

```json
{
  "sources": [
    {
      "name": "Kyle Poyar",
      "display_name": "Kyle Poyar / Growth Unhinged",
      "type": "substack",
      "url": "https://www.growthunhinged.com/archive",
      "scrape_method": "firecrawl_scrape",
      "priority": "high",
      "tags": ["pricing", "plg", "pls", "benchmarks"]
    },
    {
      "name": "Elena Verna",
      "display_name": "Elena Verna / Growth Scoop",
      "type": "substack",
      "url": "https://www.elenaverna.com/archive",
      "scrape_method": "firecrawl_scrape",
      "priority": "high",
      "tags": ["activation", "retention", "plg", "b2b"]
    },
    {
      "name": "Demand Curve",
      "display_name": "Demand Curve",
      "type": "blog",
      "url": "https://www.demandcurve.com/blog",
      "scrape_method": "firecrawl_scrape",
      "priority": "high",
      "tags": ["marketing", "growth-tactics", "conversion"]
    }
    // ... 10+ autres sources
  ],
  
  "scraping_config": {
    "lookback_days": 7,
    "max_articles_per_source": 5,
    "min_article_length": 500,
    "language_filter": ["en", "fr"]
  },
  
  "balance_rules": {
    "min_sources_represented": 10,
    "max_articles_per_source": 3,
    "total_articles": 25
  }
}
```

---

## ğŸ” Variables d'Environnement: `.env`

```bash
# APIs pour scraping (MCP tools sont dÃ©jÃ  configurÃ©s dans Warp)
ANTHROPIC_API_KEY=your_key_here  # Pour traitement IA via Claude

# Optionnel: Gmail API (pour scraper emails directement)
GMAIL_CLIENT_ID=your_gmail_client_id
GMAIL_CLIENT_SECRET=your_gmail_secret

# Configuration
OUTPUT_DIR=./output/newsletters
CACHE_DIR=./cache
LOG_LEVEL=INFO
```

---

## ğŸš€ Utilisation

### Mode Interactif (RecommandÃ© pour dÃ©buter)
```bash
# Installation
pip install -r requirements.txt

# GÃ©nÃ©ration interactive
python scripts/newsletter_generator.py --interactive

# Le script vous demandera:
# 1. Date de fin de semaine? [2025-10-26]
# 2. Nombre minimum d'articles? [25]
# 3. Mode test ou production? [test]
```

### Mode Automatique
```bash
# GÃ©nÃ©ration pour la semaine derniÃ¨re
python scripts/newsletter_generator.py --auto

# GÃ©nÃ©ration avec date spÃ©cifique
python scripts/newsletter_generator.py --end-date 2025-10-26

# Mode dry-run (affiche rÃ©sultats sans gÃ©nÃ©rer HTML)
python scripts/newsletter_generator.py --dry-run
```

### Sortie du script
```
ğŸ“¥ Collecte des sources (13 sources configurÃ©es)...
  âœ“ Kyle Poyar: 3 articles trouvÃ©s
  âœ“ Elena Verna: 2 articles trouvÃ©s
  âœ“ Demand Curve: 4 articles trouvÃ©s
  âœ“ Maja Voje: 2 articles trouvÃ©s
  ... (9+ autres sources)
  
  Total: 47 articles collectÃ©s

ğŸ¤– Traitement IA (rÃ©sumÃ©, traduction, catÃ©gorisation)...
  â³ Traitement de 47 articles... [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  âœ“ 47/47 articles traitÃ©s

âš–ï¸ Ã‰quilibrage des sources...
  â€¢ Algorithme de distribution appliquÃ©
  â€¢ 12 sources reprÃ©sentÃ©es
  â€¢ Maximum 3 articles par source respectÃ©
  
  SÃ©lection finale:
    - Critique: 8 articles
    - Important: 8 articles  
    - Bon Ã  Savoir: 9 articles
  Total: 25 articles

ğŸ“° GÃ©nÃ©ration HTML...
  âœ“ Template chargÃ©
  âœ“ DonnÃ©es injectÃ©es
  âœ“ Footer gÃ©nÃ©rÃ©
  
  Fichier crÃ©Ã©: output/newsletters/growth-weekly-2025-10-26.html

âœ… Newsletter gÃ©nÃ©rÃ©e avec succÃ¨s!

ğŸ“Š Statistiques:
  - Articles collectÃ©s: 47
  - Articles utilisÃ©s: 25
  - Sources reprÃ©sentÃ©es: 12/13
  - DurÃ©e totale: 2m 34s
```

---

## ğŸ¨ Algorithme d'Ã‰quilibrage des Sources

```python
def balance_sources(articles, min_items=25, max_per_source=3):
    """
    Assure une reprÃ©sentation Ã©quilibrÃ©e de toutes les sources
    
    Algorithme:
    1. Trie tous les articles par score (dÃ©croissant)
    2. Groupe par source
    3. Round-robin selection:
       - Pour chaque source (dans l'ordre de prioritÃ©)
       - Prend le meilleur article non encore sÃ©lectionnÃ©
       - Continue jusqu'Ã  avoir min_items articles
    4. Respecte la contrainte max_per_source
    """
    selected = []
    source_counts = defaultdict(int)
    
    # Trie par score
    sorted_articles = sorted(articles, key=lambda x: x['score'], reverse=True)
    
    # Round-robin par source
    source_pool = group_by_source(sorted_articles)
    
    while len(selected) < min_items:
        for source in source_pool:
            if source_counts[source] < max_per_source:
                if article := get_next_article(source_pool[source]):
                    selected.append(article)
                    source_counts[source] += 1
    
    return selected
```

**Exemple de distribution:**
```
Kyle Poyar: â–ˆâ–ˆâ–ˆ (3 articles)
Elena Verna: â–ˆâ–ˆâ–ˆ (3 articles)
Demand Curve: â–ˆâ–ˆ (2 articles)
Maja Voje: â–ˆâ–ˆ (2 articles)
TLDR Marketing: â–ˆâ–ˆ (2 articles)
Indie Hackers: â–ˆâ–ˆ (2 articles)
Userpilot: â–ˆâ–ˆ (2 articles)
... (autres sources)
```

---

## ğŸ”„ Workflow Hebdomadaire AutomatisÃ© (Optionnel)

### GitHub Actions: `.github/workflows/weekly_newsletter.yml`

```yaml
name: Generate Weekly Newsletter

on:
  schedule:
    # Tous les lundis Ã  9h00 UTC (10h Paris)
    - cron: '0 9 * * 1'
  workflow_dispatch:  # Permet dÃ©clenchement manuel

jobs:
  generate:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Generate newsletter
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python scripts/newsletter_generator.py --auto
      
      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v5
        with:
          commit-message: 'Newsletter: Growth Weekly for week ending ${{ env.WEEK_END }}'
          branch: newsletter/auto-${{ env.WEEK_END }}
          title: 'ğŸ“° Growth Weekly - ${{ env.WEEK_END }}'
          body: |
            Newsletter gÃ©nÃ©rÃ©e automatiquement pour la semaine se terminant le ${{ env.WEEK_END }}.
            
            ğŸ“Š Statistiques:
            - Articles collectÃ©s: ${{ env.STATS_COLLECTED }}
            - Articles sÃ©lectionnÃ©s: ${{ env.STATS_SELECTED }}
            - Sources reprÃ©sentÃ©es: ${{ env.STATS_SOURCES }}
            
            Merci de reviewer avant merge!
```

**Avantages:**
- âœ… ExÃ©cution automatique chaque lundi
- âœ… CrÃ©e une PR pour review avant publication
- âœ… Historique complet dans GitHub
- âœ… Peut Ãªtre dÃ©clenchÃ© manuellement si besoin

---

## ğŸ“ˆ AmÃ©liorations Futures

### Phase 1 (Court terme)
- âœ… Cache intelligent (Ã©vite re-scraping)
- âœ… Logs dÃ©taillÃ©s avec timestamps
- âœ… Mode dry-run pour tests
- âœ… Validation des donnÃ©es avant gÃ©nÃ©ration

### Phase 2 (Moyen terme)
- ğŸ”„ Dashboard web pour curation manuelle
- ğŸ”„ IntÃ©gration Gmail API pour newsletters email
- ğŸ”„ A/B testing des catÃ©gorisations
- ğŸ”„ Analytics: tracking des clics/lectures

### Phase 3 (Long terme)
- ğŸš€ ML model pour catÃ©gorisation automatique
- ğŸš€ DÃ©tection automatique de nouvelles sources
- ğŸš€ Envoi email automatique (Mailchimp/SendGrid)
- ğŸš€ Archive publique avec search

---

## ğŸ› ï¸ DÃ©pendances: `requirements.txt`

```txt
# Core
python>=3.11

# Web scraping
requests>=2.31.0
beautifulsoup4>=4.12.0

# APIs
anthropic>=0.18.0  # Pour Claude API (rÃ©sumÃ©s/traductions)
openai>=1.12.0     # Alternative si prÃ©fÃ©rÃ©

# Data processing
pandas>=2.2.0
python-dateutil>=2.8.2

# Configuration
python-dotenv>=1.0.0
pyyaml>=6.0.1

# Utils
tqdm>=4.66.0       # Progress bars
loguru>=0.7.0      # Better logging
```

---

## ğŸ“ Comment Commencer

1. **Lisez ce document** (vous y Ãªtes âœ…)
2. **Validez l'approche** - Est-ce que Ã§a correspond Ã  vos besoins?
3. **Je crÃ©e les fichiers** - Scripts Python + config + template
4. **Vous testez** - GÃ©nÃ©ration de la premiÃ¨re newsletter automatiquement
5. **ItÃ©rations** - On ajuste selon vos retours
6. **Automatisation** - GitHub Actions si souhaitÃ©

---

## ğŸ’¡ Avantages de cette Architecture

âœ… **Modulaire**: Chaque composant est indÃ©pendant  
âœ… **Testable**: Mode dry-run pour valider avant gÃ©nÃ©ration  
âœ… **Extensible**: Facile d'ajouter de nouvelles sources  
âœ… **Maintenable**: Configuration centralisÃ©e en JSON  
âœ… **Automatisable**: GitHub Actions ready  
âœ… **Transparent**: Logs dÃ©taillÃ©s Ã  chaque Ã©tape  

---

## â“ Questions FrÃ©quentes

**Q: Combien de temps prend la gÃ©nÃ©ration?**  
R: ~2-3 minutes pour scraper 13 sources et traiter 25 articles.

**Q: Ã‡a coÃ»te combien en API calls?**  
R: ~$0.20-0.30 par newsletter (coÃ»t Claude API pour rÃ©sumÃ©s).

**Q: Peut-on personnaliser les catÃ©gories?**  
R: Oui, facilement dans `config/prompts.json`.

**Q: Comment ajouter une nouvelle source?**  
R: Un seul ajout dans `config/sources.json`.

**Q: Ã‡a marche sans les MCP tools?**  
R: Oui, fallback sur `requests` + `BeautifulSoup` si besoin.

---

**PrÃªt pour que je crÃ©e tous les fichiers? ğŸš€**
